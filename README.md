# Project summary

This is the fourth project of Udacitys Data Engineering Nanodegree. For this project, I am trying to help a music streaming startup company called Sparkify to move their data on Spark as heir existing data warehouse can not handle the massive data resources efficiently any longer. With this project, they can now analyse their data in a distributed way in-memory. This leads to huge speedups comparing to the existing approach and allows them to keep track of their clients' behavior easily. Moreover, this data lake automates the entire data fusion process combining multiple data sources from AWS S3 into structured data. Also, the structured data gets then stored on AWS S3 again, so Sparkify can use it for further analysis.

## Data Sources

Data resides in two directories that contain files in JSON format:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`

The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

The log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.


## Generated tables

| name | type | description | columns |
| ---- | ---- | ----------- | ------- |
| songplays | fact table | records in log data associated with song plays i.e. records with page NextSong | songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent |
| users | dimension table | users in the app | user_id, first_name, last_name, gender, level |
| songs | dimension table | songs in music database | song_id, title, artist_id, year, duration |
| artists | dimension table |  artists in music database | artist_id, name, location, lattitude, longitude |
| time | dimension table | timestamps of records in songplays broken down into specific units | start_time, hour, day, week, month, year, weekday |

## ETL pipeline

The ETL pipeline (`etl.py`) loads the S3 data sources into Spark dataframes, aggregrates and transforms the data into the described schema and writes the data back to S3 in the parquet format.

## Instructions

1. Create an AWS IAM role with S3 read and write access.
2. Enter the IAM's credentials in the `dl.cfg` configuration file.
3. Create an S3 bucket and enter the URL to the bucket in `etl.py` as the value of output_data.
4. Run `python3 etl.py` to process the data and store it on your created S3 bucket.